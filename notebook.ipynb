{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNb3XMFPTK38ahdLdGcmuwH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JacobAshoo/NNFS/blob/main/notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "TODO:\n",
        "\n",
        "padding\n",
        "\n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "SmAEEBmJxuo1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "eca35021-451c-454d-deae-bd9572076815"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nTODO:\\n\\npadding\\n\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dill\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision.datasets\n",
        "import dill\n",
        "import pickle\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn.functional as F\n",
        "from math import cos, pi\n",
        "import wandb\n",
        "\n",
        "torch.manual_seed(42)\n"
      ],
      "metadata": {
        "id": "T-Q862boZl9J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df7f69ae-b901-4188-c5d8-8cae51613d33"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (0.3.9)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x79704e7123b0>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(torch.cuda.device_count()):\n",
        "  print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")"
      ],
      "metadata": {
        "id": "-JowJiltlF5S"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!wandb login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51R0EHi2REfM",
        "outputId": "c0ca718e-9b60-4db1-b232-8e90bf9c0d06"
      },
      "execution_count": 269,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjacob-ashoo\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Relu:\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def __call__(self, x):\n",
        "    self.x = x\n",
        "    return torch.maximum(x, torch.tensor(0.0, device=x.device))\n",
        "\n",
        "  def backward(self, grad):\n",
        "    return grad * (self.x > 0).float()\n",
        "\n",
        "\n",
        "\n",
        "class Softmax:\n",
        "  def __init__(self, dim=-1, device=None):\n",
        "    self.dim = dim\n",
        "    self.device = device if device else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  def __call__(self, x):\n",
        "    x = x.to(self.device)\n",
        "    x_max = torch.max(x, dim=self.dim, keepdim=True)[0]\n",
        "    exp_x = torch.exp(x - x_max)\n",
        "    sum_exp_x = torch.sum(exp_x, dim=self.dim, keepdim=True)\n",
        "    self.softmax_output = exp_x / sum_exp_x\n",
        "    return self.softmax_output\n",
        "\n",
        "  def backward(self, grad_output):\n",
        "    batch_size, num_classes = self.softmax_output.shape\n",
        "    eye = torch.eye(num_classes, device=self.device).unsqueeze(0)\n",
        "    softmax_diag = self.softmax_output.unsqueeze(2) * eye\n",
        "    softmax_outer = torch.matmul(self.softmax_output.unsqueeze(2), self.softmax_output.unsqueeze(1))\n",
        "    jacobian = softmax_diag - softmax_outer\n",
        "    grad_input = torch.matmul(jacobian, grad_output.unsqueeze(2)).squeeze(2)\n",
        "    return grad_input\n"
      ],
      "metadata": {
        "id": "_Q0ry4SqY1lG"
      },
      "execution_count": 270,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossEntropy:\n",
        "  def __init__(self, device=None):\n",
        "    self.device = device\n",
        "    self.l2_reg = False\n",
        "    self.l = 0.01\n",
        "    self.weights = None\n",
        "    self.softmax = Softmax(dim=-1, device=self.device)\n",
        "\n",
        "  def __call__(self, logits, y, l2_reg=False, l=0.01, weights=None):\n",
        "    self.l2_reg = l2_reg\n",
        "    self.l = l\n",
        "    self.weights = weights if weights is not None else []\n",
        "\n",
        "    probs = self.softmax(logits)\n",
        "    self.probs = torch.clamp(probs, min=1e-8)\n",
        "\n",
        "    loss = -torch.mean(torch.sum(y * torch.log(self.probs), dim=1))\n",
        "\n",
        "    if self.l2_reg and self.weights:\n",
        "      loss += (self.l / 2) * sum(torch.sum(w ** 2) for w in self.weights)\n",
        "\n",
        "    return loss\n",
        "\n",
        "  def backward(self, logits, y):\n",
        "    batch_size = logits.shape[0]\n",
        "    grad = (self.probs - y) / batch_size\n",
        "\n",
        "    if self.l2_reg and self.weights:\n",
        "      for w in self.weights:\n",
        "        w.grad = self.l * w\n",
        "\n",
        "    return grad"
      ],
      "metadata": {
        "id": "ATe5r3z4qHY5"
      },
      "execution_count": 271,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 272,
      "metadata": {
        "id": "9SJZW5TCYXNV"
      },
      "outputs": [],
      "source": [
        "class Linear:\n",
        "  def __init__(self, input_size, output_size, activation, device=None, dropout=1.0):\n",
        "    self.device = device\n",
        "    self.input_size = input_size\n",
        "    self.output_size = output_size\n",
        "    self.w = torch.randn(output_size, input_size, device=self.device) * .1\n",
        "    self.b = torch.zeros(output_size, device=self.device)\n",
        "    self.activation = activation\n",
        "    self.dropout = dropout\n",
        "\n",
        "  def __call__(self, x, training=True):\n",
        "    self.x = x.to(self.device)\n",
        "    self.z = torch.matmul(self.x, self.w.T) + self.b\n",
        "    self.a = self.activation(self.z)\n",
        "\n",
        "\n",
        "    if training and self.dropout < 1.0:\n",
        "      mask = (torch.rand(self.a.shape, device=self.device) < self.dropout).float()\n",
        "      self.a *= mask\n",
        "      self.a /= self.dropout\n",
        "\n",
        "    return self.a\n",
        "\n",
        "  def backward(self, grad):\n",
        "    if type(self.activation) == Softmax:\n",
        "      grad = self.activation.backward(grad)\n",
        "    dw = torch.matmul(grad.T, self.x)\n",
        "    db = torch.sum(grad, dim=0)\n",
        "    dx = torch.matmul(grad, self.w)\n",
        "    return dx, dw, db"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Conv2D:\n",
        "  def __init__(self, in_channels, out_channels, kernel_size, activation, stride=1, padding=0, device=None):\n",
        "    self.device = device\n",
        "    self.in_channels = in_channels\n",
        "    self.out_channels = out_channels\n",
        "    self.kernel_size = kernel_size\n",
        "    self.activation = activation\n",
        "    self.stride = stride\n",
        "    self.padding = padding\n",
        "    self.w = torch.randn(out_channels, in_channels, kernel_size, kernel_size, device=self.device) * 0.1\n",
        "    self.b = torch.zeros(out_channels, device=self.device)\n",
        "    self.original_shape = None\n",
        "\n",
        "  def __call__(self, x):\n",
        "    return self.forward(x)\n",
        "\n",
        "  def forward(self, x):\n",
        "    self.original_shape = x.shape\n",
        "    self.x = x.to(self.device)\n",
        "    batch_size, in_channels, in_height, in_width = x.shape\n",
        "\n",
        "    padded_x = F.pad(self.x, (self.padding, self.padding, self.padding, self.padding))\n",
        "\n",
        "    out_height = (in_height + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
        "    out_width = (in_width + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
        "\n",
        "    x_unfolded = F.unfold(padded_x, kernel_size=self.kernel_size, stride=self.stride).to(self.device)\n",
        "    w_reshaped = self.w.view(self.out_channels, -1)\n",
        "\n",
        "    out = torch.matmul(w_reshaped, x_unfolded) + self.b.view(-1, 1)\n",
        "    out = out.view(batch_size, self.out_channels, out_height, out_width)\n",
        "    self.a = self.activation(out)\n",
        "    return self.a\n",
        "\n",
        "  def backward(self, grad):\n",
        "    if grad.dim() == 2:\n",
        "      batch_size = self.original_shape[0]\n",
        "      out_height = (self.original_shape[2] + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
        "      out_width = (self.original_shape[3] + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
        "      grad = grad.reshape(batch_size, self.out_channels, out_height, out_width)\n",
        "\n",
        "    batch_size, _, out_height, out_width = grad.shape\n",
        "    grad_flattened = grad.reshape(batch_size, self.out_channels, -1)\n",
        "    padded_x = F.pad(self.x, (self.padding, self.padding, self.padding, self.padding))\n",
        "    x_unfolded = F.unfold(padded_x, kernel_size=self.kernel_size, stride=self.stride)\n",
        "\n",
        "    dw = torch.matmul(grad_flattened, x_unfolded.transpose(1, 2))\n",
        "    dw = dw.sum(dim=0).view(self.w.shape)\n",
        "    db = grad_flattened.sum(dim=(0, 2))\n",
        "\n",
        "    w_reshaped = self.w.view(self.out_channels, -1)\n",
        "    dx_unfolded = torch.matmul(w_reshaped.T, grad_flattened)\n",
        "    dx = F.fold(dx_unfolded, output_size=(self.original_shape[2] + 2 * self.padding, self.original_shape[3] + 2 * self.padding), kernel_size=self.kernel_size, stride=self.stride)\n",
        "\n",
        "    if self.padding > 0:\n",
        "      dx = dx[:, :, self.padding:-self.padding, self.padding:-self.padding]\n",
        "\n",
        "    return dx, dw, db\n"
      ],
      "metadata": {
        "id": "sfahZ_XXikSX"
      },
      "execution_count": 274,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GradientDescent:\n",
        "  def __init__(self, learning_rate, beta):\n",
        "    self.lr = learning_rate\n",
        "    self.beta1 = beta\n",
        "\n",
        "  def optimize(self, dw, db, layer):\n",
        "    if not hasattr(self, 'vw') or self.vw.shape != dw.shape:\n",
        "        self.vw = torch.zeros_like(dw)\n",
        "        self.vb = torch.zeros_like(db)\n",
        "\n",
        "    self.vw = (self.beta1 * self.vw) + ((1 - self.beta1) * dw)\n",
        "    self.vb = (self.beta1 * self.vb) + ((1 - self.beta1) * db)\n",
        "\n",
        "    layer.w -= self.lr * self.vw\n",
        "    layer.b -= self.lr * self.vb"
      ],
      "metadata": {
        "id": "uQGPX94B1Jow"
      },
      "execution_count": 275,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Adam:\n",
        "  def __init__(self, learning_rate, beta1, beta2):\n",
        "    self.lr = learning_rate\n",
        "    self.beta1 = beta1\n",
        "    self.beta2 = beta2\n",
        "    self.t = 1\n",
        "\n",
        "  def optimize(self, dw, db, layer):\n",
        "    if not hasattr(self, 'vw') or self.vw.shape != dw.shape:\n",
        "        self.vw = torch.zeros_like(dw)\n",
        "        self.vb = torch.zeros_like(db)\n",
        "        self.sw = torch.zeros_like(dw)\n",
        "        self.sb = torch.zeros_like(db)\n",
        "\n",
        "    self.vw = (self.beta1 * self.vw) + ((1 - self.beta1) * dw)\n",
        "    self.vb = (self.beta1 * self.vb) + ((1 - self.beta1) * db)\n",
        "    self.sw = (self.beta2 * self.sw) + ((1 - self.beta2) * dw * dw)\n",
        "    self.sb = (self.beta2 * self.sb) + ((1 - self.beta2) * db * db)\n",
        "\n",
        "    self.vw = self.vw/(1-pow(self.beta1,self.t))\n",
        "    self.vb = self.vb/(1-pow(self.beta1,self.t))\n",
        "    self.sw = self.sw/(1-pow(self.beta2,self.t))\n",
        "    self.sb = self.sb/(1-pow(self.beta2,self.t))\n",
        "\n",
        "\n",
        "    layer.w -= self.lr * (self.vw/(torch.sqrt(self.sw) + 1e-3))\n",
        "    layer.b -= self.lr * (self.vb/(torch.sqrt(self.sb) + 1e-3))\n",
        "\n",
        "    self.t += 1\n"
      ],
      "metadata": {
        "id": "m__MDiP0OxUo"
      },
      "execution_count": 276,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def step_decay(epoch, initial_lr=1e-3):\n",
        "    if epoch < 3:\n",
        "        return initial_lr * min(1.0, float(epoch + 1) / 5)\n",
        "    elif epoch < 10:\n",
        "        return initial_lr\n",
        "    elif epoch < 15:\n",
        "        return initial_lr * 0.5\n",
        "    else:\n",
        "        return initial_lr * 0.01\n",
        "\n",
        "def cosine_decay(epoch, num_epochs, initial_lr):\n",
        "  return .5 *initial_lr * (1 + cos((epoch * pi)/num_epochs))\n",
        "\n",
        "def warmup(epoch, lr):\n",
        "  if epoch < 5:\n",
        "    return lr * (epoch + 1) / 5\n",
        "  else:\n",
        "    return lr"
      ],
      "metadata": {
        "id": "C39GXyYeZ0dU"
      },
      "execution_count": 277,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FCNN:\n",
        "  def __init__(self, device=None):\n",
        "    self.l1 = Conv2D(3, 32, 3, Relu(), stride=3, device=device, padding=1)\n",
        "    self.l2 = Conv2D(32, 64, 3, Relu(), stride=3, device=device, padding=1)\n",
        "    self.l3 = Linear(1024, 256, Relu(), device=device)\n",
        "    self.l4 = Linear(256, 10, lambda x:x, device=device)\n",
        "\n",
        "    self.layers = [self.l1, self.l2, self.l3, self.l4]\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.l1(x)\n",
        "    x = self.l2(x)\n",
        "    x = torch.flatten(x, 1)\n",
        "    x = self.l3(x)\n",
        "    x = self.l4(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "  def __call__(self, x):\n",
        "    return self.forward(x)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_gN-0fQpaih4"
      },
      "execution_count": 278,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NN:\n",
        "  def __init__(self, conv_layers, fc_layers):\n",
        "    self.layers = []\n",
        "    for layer in conv_layers:\n",
        "      #[in, out, kernal_size, stride, padding]\n",
        "      self.layers.append(Conv2D(layer[0], layer[1], layer[2], Relu(), stride=layer[3], padding=layer[4]))\n",
        "    for layer in fc_layers:\n",
        "      #[input_size, output_size, dropout]\n",
        "      self.layers.append(Linear(layer[0], layer[1], Relu(), dropout=layer[2]))\n",
        "    self.layers[-1].activation = lambda x:x\n",
        "\n",
        "  def forward(self, x):\n",
        "    for layer in self.layers:\n",
        "      if(type(layer)==Linear and x.dim()>2):\n",
        "        x = torch.flatten(x, 1)\n",
        "      x = layer(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "  def __call__(self, x):\n",
        "    return self.forward(x)"
      ],
      "metadata": {
        "id": "TZ-KzrUdyH9E"
      },
      "execution_count": 279,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(model, name):\n",
        "  with open(f\"{name}.dill\", \"wb\") as f:\n",
        "    dill.dump(model, f)\n",
        "\n"
      ],
      "metadata": {
        "id": "XZE0SfjOx0Z8"
      },
      "execution_count": 280,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def import_data():\n",
        "  data = torchvision.datasets.CIFAR10(root=\"/content\",download=True)\n",
        "\n",
        "  xs = []\n",
        "  labels = []\n",
        "  for i in range(1,6):\n",
        "    with open(f\"/content/cifar-10-batches-py/data_batch_{i}\", 'rb') as f:\n",
        "      dict = pickle.load(f, encoding='bytes')\n",
        "      xs.append(dict[b'data'])\n",
        "      labels += dict[b'labels']\n",
        "\n",
        "  labels = np.array(labels).reshape(-1,1)\n",
        "  encoder = OneHotEncoder(sparse_output=False)\n",
        "  y = encoder.fit_transform(labels)\n",
        "\n",
        "  xs = np.array(xs)\n",
        "  x = xs.reshape(50000, 3, 32, 32) / 255\n",
        "\n",
        "  return x, y\n",
        "\n",
        "\n",
        "def load_data(x, y, device=None):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(\n",
        "      x, y, test_size=0.166, random_state=42)\n",
        "  X_train = torch.tensor(X_train, dtype=torch.float32, device=device)\n",
        "  y_train = torch.tensor(y_train, dtype=torch.float32, device=device)\n",
        "  X_test = torch.tensor(X_test, dtype=torch.float32, device=device)\n",
        "  y_test = torch.tensor(y_test, dtype=torch.float32, device=device)\n",
        "\n",
        "  train_dataset = TensorDataset(X_train, y_train)\n",
        "  test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "  batch_size = 32\n",
        "  train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "  test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  return train_loader, test_loader"
      ],
      "metadata": {
        "id": "OaZtRznWPE8F"
      },
      "execution_count": 281,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title train\n",
        "\n",
        "\n",
        "def train(model, epochs, lr, loss_function, optimizer, name, train_loader, test_loader, decay_algo=None, use_wandb=True, save=True, device=None):\n",
        "  softmax = Softmax(device=device)\n",
        "  if use_wandb:\n",
        "    wandb.init(project=\"ConvNet\", entity=\"jacob-ashoo\", name=name, config={\n",
        "        \"name\" : name,\n",
        "        \"learning_rate\":lr,\n",
        "        \"epochs\":epochs,\n",
        "        \"optimizer\" : type(optimizer).__name__,\n",
        "        \"beta1\" : optimizer.beta1,\n",
        "        \"beta2\" : optimizer.beta2 if type(optimizer)==Adam else 0,\n",
        "        \"l2_reg\" : \"l2\" in name.lower(),\n",
        "        \"l\" : loss_function.l if \"l2\" in name.lower() else 0,\n",
        "        \"dropout\" : \"dropout\" in name.lower(),\n",
        "        \"decay_algo\" : decay_algo\n",
        "    })\n",
        "  for epoch in range(epochs):\n",
        "    losses = []\n",
        "    n_total = 0\n",
        "    n_correct = 0\n",
        "\n",
        "    if decay_algo == \"cosine\":\n",
        "      optimizer.lr = cosine_decay(epoch, epochs, lr)\n",
        "    if decay_algo == \"step\":\n",
        "      optimizer.lr = step_decay(epoch, lr)\n",
        "    if(decay_algo==\"warmup\"):\n",
        "      optimizer.lr = warmup(epoch, lr)\n",
        "\n",
        "    for (iteration, (x,y)) in enumerate(train_loader):\n",
        "      ypred = model(x)\n",
        "      loss = loss_function(ypred, y)\n",
        "      losses.append(loss)\n",
        "\n",
        "      n_total += y.size(dim=0)\n",
        "      ypred = softmax(ypred)\n",
        "      guesses = torch.argmax(ypred, dim=1)\n",
        "      truths = torch.argmax(y, dim=1)\n",
        "\n",
        "      for i in range(len(guesses)):\n",
        "        if(guesses[i]==truths[i]):\n",
        "          n_correct += 1\n",
        "\n",
        "\n",
        "      # Backpropagation\n",
        "      grad = loss_function.backward(ypred, y)\n",
        "      dx = grad\n",
        "\n",
        "      for layer in reversed(model.layers):\n",
        "        dx, dw, db = layer.backward(dx)\n",
        "        optimizer.optimize(dw, db, layer)\n",
        "\n",
        "    loss = sum(losses)/len(losses)\n",
        "    accuracy = n_correct/n_total\n",
        "\n",
        "    print(f\"epoch {epoch} loss: {loss} accuracy: {accuracy}\")\n",
        "    if use_wandb:\n",
        "      wandb.log({\"train_loss\":loss, \"accuracy\":accuracy})\n",
        "\n",
        "  losses = []\n",
        "  n_total = 0\n",
        "  n_correct = 0\n",
        "  for (iteration, (x,y)) in enumerate(test_loader):\n",
        "      ypred = model(x)\n",
        "      loss = loss_function(ypred, y)\n",
        "      losses.append(loss)\n",
        "\n",
        "      n_total += batch_size\n",
        "      ypred = softmax(ypred)\n",
        "      guesses = torch.argmax(ypred, dim=1)\n",
        "      truths = torch.argmax(y, dim=1)\n",
        "\n",
        "      for i in range(len(guesses)):\n",
        "        if(guesses[i]==truths[i]):\n",
        "          n_correct += 1\n",
        "\n",
        "  loss = sum(losses)/len(losses)\n",
        "  accuracy = n_correct/n_total\n",
        "  if save:\n",
        "    save_model(model, name)\n",
        "  if use_wandb:\n",
        "    wandb.log({\"test_loss\":loss, \"test_accuracy\":accuracy})\n",
        "    wandb.save(f\"{name}.dill\")\n",
        "    wandb.finish()\n",
        "\n"
      ],
      "metadata": {
        "id": "KZ28sDhHitJL"
      },
      "execution_count": 282,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Ablation\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using {device}\")\n",
        "\n",
        "x, y = import_data()\n",
        "train_loader, test_loader = load_data(x, y, device=device)\n",
        "\n",
        "\n",
        "gd_lr = 5e-3\n",
        "adam_lr = 1e-5\n",
        "epochs = 30\n",
        "\n",
        "\n",
        "\n",
        "model = FCNN(device=device)\n",
        "train(model, epochs, gd_lr, CrossEntropy(device=device), Adam(adam_lr, .9, .999), \"Adam_test\", train_loader, test_loader, decay_algo=\"step\", use_wandb=False, save=False, device=device)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vOFFzqEhq6tE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "28062d22-3ddf-4853-e577-6d5659735aca"
      },
      "execution_count": 283,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu\n",
            "Files already downloaded and verified\n",
            "epoch 0 loss: 1.8685208559036255 accuracy: 0.33700239808153476\n",
            "epoch 1 loss: 2.017793655395508 accuracy: 0.32016786570743405\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-283-39c4168c2645>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgd_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCrossEntropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madam_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m.999\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Adam_test\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay_algo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"step\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_wandb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-282-0d1b46ee2076>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, epochs, lr, loss_function, optimizer, name, train_loader, test_loader, decay_algo, use_wandb, save, device)\u001b[0m\n\u001b[1;32m     50\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mdx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-276-7298bcc3d9f2>\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, dw, db, layer)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdb\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvw\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvb\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msw\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FJiD44n1k68J"
      },
      "execution_count": 296,
      "outputs": []
    }
  ]
}