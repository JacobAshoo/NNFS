{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNxkOzOx22HYLeuvujSEiOa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JacobAshoo/NNFS/blob/main/notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dill\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision.datasets\n",
        "import dill\n",
        "import pickle\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn.functional as F\n",
        "from math import cos, pi\n",
        "import wandb\n",
        "\n",
        "torch.manual_seed(42)\n"
      ],
      "metadata": {
        "id": "T-Q862boZl9J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9786fea4-1182-4a8e-f586-66056a7465cb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (0.3.9)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x78ddf02fe350>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!wandb login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51R0EHi2REfM",
        "outputId": "c5755a2e-9885-4f79-c4e2-67af139b8ddb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.8)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.25.6)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.10.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.22.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjacob-ashoo\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Relu:\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def __call__(self, x):\n",
        "    self.x = x\n",
        "    return torch.maximum(x, torch.tensor(0.0, device=x.device))\n",
        "\n",
        "  def backward(self, grad):\n",
        "    return grad * (self.x > 0).float()\n",
        "\n",
        "\n",
        "\n",
        "class Softmax:\n",
        "  def __init__(self, dim=-1, device=None):\n",
        "    self.dim = dim\n",
        "    self.device = device if device else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  def __call__(self, x):\n",
        "    x = x.to(self.device)\n",
        "    x_max = torch.max(x, dim=self.dim, keepdim=True)[0]\n",
        "    exp_x = torch.exp(x - x_max)\n",
        "    sum_exp_x = torch.sum(exp_x, dim=self.dim, keepdim=True)\n",
        "    self.softmax_output = exp_x / sum_exp_x\n",
        "    return self.softmax_output\n",
        "\n",
        "  def backward(self, grad_output):\n",
        "    batch_size, num_classes = self.softmax_output.shape\n",
        "    eye = torch.eye(num_classes, device=self.device).unsqueeze(0)\n",
        "    softmax_diag = self.softmax_output.unsqueeze(2) * eye\n",
        "    softmax_outer = torch.matmul(self.softmax_output.unsqueeze(2), self.softmax_output.unsqueeze(1))\n",
        "    jacobian = softmax_diag - softmax_outer\n",
        "    grad_input = torch.matmul(jacobian, grad_output.unsqueeze(2)).squeeze(2)\n",
        "    return grad_input\n"
      ],
      "metadata": {
        "id": "_Q0ry4SqY1lG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossEntropy:\n",
        "  def __init__(self):\n",
        "    self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    self.l2_reg = False\n",
        "    self.l = 0.01\n",
        "    self.weights = None\n",
        "    self.softmax = Softmax(dim=-1, device=self.device)\n",
        "\n",
        "  def __call__(self, logits, y, l2_reg=False, l=0.01, weights=None):\n",
        "    self.l2_reg = l2_reg\n",
        "    self.l = l\n",
        "    self.weights = weights if weights is not None else []\n",
        "\n",
        "    probs = self.softmax(logits)\n",
        "    self.probs = torch.clamp(probs, min=1e-8)\n",
        "\n",
        "    loss = -torch.mean(torch.sum(y * torch.log(self.probs), dim=1))\n",
        "\n",
        "    if self.l2_reg and self.weights:\n",
        "      loss += (self.l / 2) * sum(torch.sum(w ** 2) for w in self.weights)\n",
        "\n",
        "    return loss\n",
        "\n",
        "  def backward(self, logits, y):\n",
        "    batch_size = logits.shape[0]\n",
        "    grad = (self.probs - y) / batch_size\n",
        "\n",
        "    if self.l2_reg and self.weights:\n",
        "      for w in self.weights:\n",
        "        w.grad = self.l * w\n",
        "\n",
        "    return grad"
      ],
      "metadata": {
        "id": "ATe5r3z4qHY5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9SJZW5TCYXNV"
      },
      "outputs": [],
      "source": [
        "class Linear:\n",
        "  def __init__(self, input_size, output_size, activation, device=None, dropout=1.0):\n",
        "    self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') if device is None else device\n",
        "    self.input_size = input_size\n",
        "    self.output_size = output_size\n",
        "    self.w = torch.randn(output_size, input_size, device=self.device) * .1\n",
        "    self.b = torch.zeros(output_size, device=self.device)\n",
        "    self.activation = activation\n",
        "    self.dropout = dropout\n",
        "\n",
        "  def __call__(self, x, training=True):\n",
        "    self.x = x.to(self.device)\n",
        "    self.z = torch.matmul(self.x, self.w.T) + self.b\n",
        "    self.a = self.activation(self.z)\n",
        "\n",
        "\n",
        "    if training and self.dropout < 1.0:\n",
        "      mask = (torch.rand(self.a.shape, device=self.device) < self.dropout).float()\n",
        "      self.a *= mask\n",
        "      self.a /= self.dropout\n",
        "\n",
        "    return self.a\n",
        "\n",
        "  def backward(self, grad):\n",
        "    if type(self.activation) == Softmax:\n",
        "      grad = self.activation.backward(grad)\n",
        "    dw = torch.matmul(grad.T, self.x)\n",
        "    db = torch.sum(grad, dim=0)\n",
        "    dx = torch.matmul(grad, self.w)\n",
        "    return dx, dw, db"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Conv2D:\n",
        "  def __init__(self, in_channels, out_channels, kernel_size, activation, stride=1):\n",
        "    self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    self.in_channels = in_channels\n",
        "    self.out_channels = out_channels\n",
        "    self.kernel_size = kernel_size\n",
        "    self.activation = activation\n",
        "    self.stride = stride\n",
        "    self.w = torch.randn(out_channels, in_channels, kernel_size, kernel_size, device=self.device) * 0.1\n",
        "    self.b = torch.zeros(out_channels, device=self.device)\n",
        "    self.original_shape = None\n",
        "\n",
        "  def __call__(self, x):\n",
        "    return self.forward(x)\n",
        "\n",
        "  def forward(self, x):\n",
        "    self.original_shape = x.shape  # Store original shape\n",
        "    self.x = x.to(self.device)  # Move input to the correct device\n",
        "    batch_size, in_channels, in_height, in_width = x.shape\n",
        "\n",
        "    out_height = (in_height - self.kernel_size) // self.stride + 1\n",
        "    out_width = (in_width - self.kernel_size) // self.stride + 1\n",
        "\n",
        "    x_unfolded = F.unfold(self.x, kernel_size=self.kernel_size, stride=self.stride).to(self.device)  # Move to device\n",
        "    w_reshaped = self.w.view(self.out_channels, -1)\n",
        "\n",
        "    out = torch.matmul(w_reshaped, x_unfolded) + self.b.view(-1, 1)\n",
        "    out = out.view(batch_size, self.out_channels, out_height, out_width)\n",
        "    self.a = self.activation(out)\n",
        "    return self.a\n",
        "\n",
        "\n",
        "  def backward(self, grad):\n",
        "    if grad.dim() == 2:\n",
        "      batch_size = self.original_shape[0]\n",
        "      out_height = (self.original_shape[2] - self.kernel_size) // self.stride + 1\n",
        "      out_width = (self.original_shape[3] - self.kernel_size) // self.stride + 1\n",
        "      grad = grad.view(batch_size, self.out_channels, out_height, out_width)\n",
        "\n",
        "    batch_size, _, out_height, out_width = grad.shape\n",
        "    grad_flattened = grad.view(batch_size, self.out_channels, -1)\n",
        "    x_unfolded = F.unfold(self.x, kernel_size=self.kernel_size, stride=self.stride)\n",
        "\n",
        "    dw = torch.matmul(grad_flattened, x_unfolded.transpose(1, 2))\n",
        "    dw = dw.sum(dim=0).view(self.w.shape)\n",
        "    db = grad_flattened.sum(dim=(0, 2))\n",
        "\n",
        "    w_reshaped = self.w.view(self.out_channels, -1)\n",
        "    dx_unfolded = torch.matmul(w_reshaped.T, grad_flattened)\n",
        "    dx = F.fold(dx_unfolded, output_size=(self.x.shape[2], self.x.shape[3]), kernel_size=self.kernel_size, stride=self.stride)\n",
        "\n",
        "    return dx, dw, db\n"
      ],
      "metadata": {
        "id": "TV9y6BR3WvuK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GradientDescent:\n",
        "  def __init__(self, learning_rate, beta):\n",
        "    self.lr = learning_rate\n",
        "    self.beta1 = beta\n",
        "\n",
        "  def optimize(self, dw, db, layer):\n",
        "    if not hasattr(self, 'vw') or self.vw.shape != dw.shape:\n",
        "        self.vw = torch.zeros_like(dw)\n",
        "        self.vb = torch.zeros_like(db)\n",
        "\n",
        "    self.vw = (self.beta1 * self.vw) + ((1 - self.beta1) * dw)\n",
        "    self.vb = (self.beta1 * self.vb) + ((1 - self.beta1) * db)\n",
        "\n",
        "    layer.w -= self.lr * self.vw\n",
        "    layer.b -= self.lr * self.vb"
      ],
      "metadata": {
        "id": "uQGPX94B1Jow"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Adam:\n",
        "  def __init__(self, learning_rate, beta1, beta2):\n",
        "    self.lr = learning_rate\n",
        "    self.beta1 = beta1\n",
        "    self.beta2 = beta2\n",
        "    self.t = 1\n",
        "\n",
        "  def optimize(self, dw, db, layer):\n",
        "    if not hasattr(self, 'vw') or self.vw.shape != dw.shape:\n",
        "        self.vw = torch.zeros_like(dw)\n",
        "        self.vb = torch.zeros_like(db)\n",
        "        self.sw = torch.zeros_like(dw)\n",
        "        self.sb = torch.zeros_like(db)\n",
        "\n",
        "    self.vw = (self.beta1 * self.vw) + ((1 - self.beta1) * dw)\n",
        "    self.vb = (self.beta1 * self.vb) + ((1 - self.beta1) * db)\n",
        "    self.sw = (self.beta2 * self.sw) + ((1 - self.beta2) * dw * dw)\n",
        "    self.sb = (self.beta2 * self.sb) + ((1 - self.beta2) * db * db)\n",
        "\n",
        "    self.vw = self.vw/(1-pow(self.beta1,self.t))\n",
        "    self.vb = self.vb/(1-pow(self.beta1,self.t))\n",
        "    self.sw = self.sw/(1-pow(self.beta2,self.t))\n",
        "    self.sb = self.sb/(1-pow(self.beta2,self.t))\n",
        "\n",
        "\n",
        "    layer.w -= self.lr * (self.vw/(torch.sqrt(self.sw) + 1e-3))\n",
        "    layer.b -= self.lr * (self.vb/(torch.sqrt(self.sb) + 1e-3))\n",
        "\n",
        "    self.t += 1\n"
      ],
      "metadata": {
        "id": "m__MDiP0OxUo"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def step_decay(epoch, initial_lr=1e-3):\n",
        "    if epoch < 3:\n",
        "        return initial_lr * min(1.0, float(epoch + 1) / 5)\n",
        "    elif epoch < 10:\n",
        "        return initial_lr\n",
        "    elif epoch < 15:\n",
        "        return initial_lr * 0.5\n",
        "    else:\n",
        "        return initial_lr * 0.01\n",
        "\n",
        "def cosine_decay(epoch, num_epochs, initial_lr):\n",
        "  return .5 *initial_lr * (1 + cos((epoch * pi)/num_epochs))\n",
        "\n",
        "def warmup(epoch, lr):\n",
        "  if epoch < 5:\n",
        "    return lr * (epoch + 1) / 5\n",
        "  else:\n",
        "    return lr"
      ],
      "metadata": {
        "id": "C39GXyYeZ0dU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FCNN:\n",
        "  def __init__(self):\n",
        "    self.l1 = Conv2D(3, 32, 3, Relu(), stride=3)\n",
        "    self.l2 = Conv2D(32, 64, 3, Relu(), stride=3)\n",
        "    self.l3 = Linear(576, 576, Relu())\n",
        "    self.l4 = Linear(576, 10, lambda x:x)\n",
        "\n",
        "    self.layers = [self.l1, self.l2, self.l3, self.l4]\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.l1(x)\n",
        "    x = self.l2(x)\n",
        "    x = torch.flatten(x, 1)\n",
        "    x = self.l3(x)\n",
        "    x = self.l4(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "  def __call__(self, x):\n",
        "    return self.forward(x)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_gN-0fQpaih4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(model, name):\n",
        "  with open(f\"{name}.dill\", \"wb\") as f:\n",
        "    dill.dump(model, f)\n",
        "\n"
      ],
      "metadata": {
        "id": "XZE0SfjOx0Z8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = torchvision.datasets.CIFAR10(root=\"/content\",download=True)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "xs = []\n",
        "labels = []\n",
        "\n",
        "for i in range(1,6):\n",
        "  with open(f\"/content/cifar-10-batches-py/data_batch_{i}\", 'rb') as f:\n",
        "    dict = pickle.load(f, encoding='bytes')\n",
        "    xs.append(dict[b'data'])\n",
        "    labels += dict[b'labels']\n",
        "\n",
        "\n",
        "labels = np.array(labels).reshape(-1,1)\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "y = encoder.fit_transform(labels)\n",
        "\n",
        "xs = np.array(xs)\n",
        "x = xs.reshape(50000, 3, 32, 32) / 255\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    x, y, test_size=0.166, random_state=42)\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32, device=device)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32, device=device)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32, device=device)\n",
        "y_test = torch.tensor(y_test, dtype=torch.float32, device=device)\n",
        "\n",
        "\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OaZtRznWPE8F",
        "outputId": "83cde995-94e4-4977-cefc-04cc44bad0e6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title train\n",
        "\n",
        "\n",
        "def train(model, epochs, lr, loss_function, optimizer, name, decay_algo=None):\n",
        "  softmax = Softmax()\n",
        "  wandb.init(project=\"ConvNet\", entity=\"jacob-ashoo\", name=name, config={\n",
        "      \"name\" : name,\n",
        "      \"optimizer\" : type(optimizer).__name__,\n",
        "      \"beta1\" : optimizer.beta1,\n",
        "      \"beta2\" : optimizer.beta2 if type(optimizer)==Adam else 0,\n",
        "      \"l2_reg\" : \"l2\" in name.lower(),\n",
        "      \"l\" : loss_function.l if \"l2\" in name.lower() else 0,\n",
        "      \"dropout\" : \"dropout\" in name.lower(),\n",
        "      \"decay_algo\" : decay_algo\n",
        "  })\n",
        "  for epoch in range(epochs):\n",
        "    losses = []\n",
        "    n_total = 0\n",
        "    n_correct = 0\n",
        "\n",
        "    if decay_algo == \"cosine\":\n",
        "      optimizer.lr = cosine_decay(epoch, epochs, lr)\n",
        "    if decay_algo == \"step\":\n",
        "      optimizer.lr = step_decay(epoch, lr)\n",
        "    if(decay_algo==\"warmup\"):\n",
        "      optimizer.lr = warmup(epoch, lr)\n",
        "\n",
        "    for (iteration, (x,y)) in enumerate(train_loader):\n",
        "      ypred = model(x)\n",
        "      loss = loss_function(ypred, y)\n",
        "      losses.append(loss)\n",
        "\n",
        "      n_total += batch_size\n",
        "      ypred = softmax(ypred)\n",
        "      guesses = torch.argmax(ypred, dim=1)\n",
        "      truths = torch.argmax(y, dim=1)\n",
        "\n",
        "      for i in range(len(guesses)):\n",
        "        if(guesses[i]==truths[i]):\n",
        "          n_correct += 1\n",
        "\n",
        "\n",
        "      # Backpropagation\n",
        "      grad = loss_function.backward(ypred, y)\n",
        "      dx = grad\n",
        "\n",
        "      for layer in reversed(model.layers):\n",
        "        dx, dw, db = layer.backward(dx)\n",
        "        optimizer.optimize(dw, db, layer)\n",
        "\n",
        "    loss = sum(losses)/len(losses)\n",
        "    accuracy = n_correct/n_total\n",
        "\n",
        "    print(f\"epoch {epoch} loss: {loss} accuracy: {accuracy}\")\n",
        "    wandb.log({\"train_loss\":loss, \"accuracy\":accuracy})\n",
        "\n",
        "  losses = []\n",
        "  n_total = 0\n",
        "  n_correct = 0\n",
        "  for (iteration, (x,y)) in enumerate(test_loader):\n",
        "      ypred = model(x)\n",
        "      loss = loss_function(ypred, y)\n",
        "      losses.append(loss)\n",
        "\n",
        "      n_total += batch_size\n",
        "      ypred = softmax(ypred)\n",
        "      guesses = torch.argmax(ypred, dim=1)\n",
        "      truths = torch.argmax(y, dim=1)\n",
        "\n",
        "      for i in range(len(guesses)):\n",
        "        if(guesses[i]==truths[i]):\n",
        "          n_correct += 1\n",
        "\n",
        "  loss = sum(losses)/len(losses)\n",
        "  accuracy = n_correct/n_total\n",
        "  wandb.log({\"test_loss\":loss, \"test_accuracy\":accuracy})\n",
        "  save_model(model, name)\n",
        "  wandb.save(f\"{name}.dill\")\n",
        "  wandb.finish()\n",
        "\n"
      ],
      "metadata": {
        "id": "KZ28sDhHitJL"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Ablation\n",
        "print(f\"Using {device}\")\n",
        "model = FCNN()\n",
        "gd_lr = 5e-3\n",
        "adam_lr = 1e-5\n",
        "loss_function = CrossEntropy()\n",
        "epochs = 20\n",
        "\n",
        "\n",
        "\n",
        "#basline adam and gcd no decay\n",
        "train(model, epochs, gd_lr, loss_function, GradientDescent(learning_rate=gd_lr, beta=.9), \"Default_GD\")\n",
        "model = FCNN()\n",
        "train(model, epochs, adam_lr, loss_function, Adam(learning_rate=adam_lr, beta1=.9, beta2=.999), \"Default_ADAM\")\n",
        "model = FCNN()\n",
        "\n",
        "#adam and gcd with step and cosine\n",
        "train(model, epochs, gd_lr, loss_function, GradientDescent(learning_rate=lr, beta=.9), \"Default_GD_Step\",decay_algo=\"step\")\n",
        "model = FCNN()\n",
        "train(model, epochs, adam_lr, loss_function, GradientDescent(learning_rate=lr, beta=.9), \"Default_GD_Cosine\",decay_algo=\"cosine\")\n",
        "model = FCNN()\n",
        "train(model, epochs, gd_lr, loss_function, Adam(learning_rate=lr, beta1=.9, beta2=.99), \"Default_Adam_Step\",decay_algo=\"step\")\n",
        "model = FCNN()\n",
        "train(model, epochs, adam_lr, loss_function, Adam(learning_rate=lr, beta1=.9, beta2=.99), \"Default_Adam_Cosine\",decay_algo=\"cosine\")\n",
        "model = FCNN()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vOFFzqEhq6tE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 620
        },
        "outputId": "56d643aa-a318-441d-fd3a-824c0eeaff19"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjacob-ashoo\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250311_181230-l664ux3s</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jacob-ashoo/ConvNet/runs/l664ux3s' target=\"_blank\">Default_GD</a></strong> to <a href='https://wandb.ai/jacob-ashoo/ConvNet' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jacob-ashoo/ConvNet' target=\"_blank\">https://wandb.ai/jacob-ashoo/ConvNet</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jacob-ashoo/ConvNet/runs/l664ux3s' target=\"_blank\">https://wandb.ai/jacob-ashoo/ConvNet/runs/l664ux3s</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0 loss: 2.272001028060913 accuracy: 0.14922833588957055\n",
            "epoch 1 loss: 2.1317193508148193 accuracy: 0.22004409509202455\n",
            "epoch 2 loss: 2.0444140434265137 accuracy: 0.2621021855828221\n",
            "epoch 3 loss: 1.9934786558151245 accuracy: 0.28599501533742333\n",
            "epoch 4 loss: 1.959000825881958 accuracy: 0.30526265337423314\n",
            "epoch 0 loss: 2.2842228412628174 accuracy: 0.1413439417177914\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-4afc573b5967>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madam_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madam_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.999\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Default_ADAM\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-b5c472005257>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, epochs, lr, loss_function, optimizer, name, decay_algo)\u001b[0m\n\u001b[1;32m     47\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mdx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-be501bc4d7e8>\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, dw, db, layer)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdb\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvw\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvb\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msw\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#adam with diferent betas and best decay function\n",
        "#train(model, epochs, lr, loss_function, Adam(learning_rate=lr, beta1=.7, beta2=.99), \"Adam\")\n",
        "#model = FCNN()\n",
        "#train(model, epochs, lr, loss_function, Adam(learning_rate=lr, beta1=.9, beta2=.8))\n",
        "#model = FCNN()\n",
        "\n"
      ],
      "metadata": {
        "id": "1ukvjTVKVHIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#regularization on best model so far\n",
        "#dropout and l2 reg diferent lambdas"
      ],
      "metadata": {
        "id": "LuV-f6QN0uvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "svo0v816WXpx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}