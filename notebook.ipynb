{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPSNXtV3NC7XG89UkDKio1q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JacobAshoo/NNFS/blob/main/notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import pickle\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ],
      "metadata": {
        "id": "T-Q862boZl9J"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Relu:\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def __call__(self, x):\n",
        "    self.x = x\n",
        "    return torch.maximum(x, torch.tensor(0.0, device=x.device))\n",
        "\n",
        "  def backward(self, grad):\n",
        "    return grad * (self.x > 0).float()\n",
        "\n",
        "\n",
        "\n",
        "class Softmax:\n",
        "  def __init__(self, dim=-1, device=None):\n",
        "    self.dim = dim\n",
        "    self.device = device if device else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  def __call__(self, x):\n",
        "    x = x.to(self.device)\n",
        "    x_max = torch.max(x, dim=self.dim, keepdim=True)[0]\n",
        "    exp_x = torch.exp(x - x_max)\n",
        "    sum_exp_x = torch.sum(exp_x, dim=self.dim, keepdim=True)\n",
        "    self.softmax_output = exp_x / sum_exp_x\n",
        "    return self.softmax_output\n",
        "\n",
        "  def backward(self, grad_output):\n",
        "    batch_size, num_classes = self.softmax_output.shape\n",
        "    eye = torch.eye(num_classes, device=self.device).unsqueeze(0)\n",
        "    softmax_diag = self.softmax_output.unsqueeze(2) * eye\n",
        "    softmax_outer = torch.matmul(self.softmax_output.unsqueeze(2), self.softmax_output.unsqueeze(1))\n",
        "    jacobian = softmax_diag - softmax_outer\n",
        "    grad_input = torch.matmul(jacobian, grad_output.unsqueeze(2)).squeeze(2)\n",
        "    return grad_input\n",
        "\n"
      ],
      "metadata": {
        "id": "_Q0ry4SqY1lG"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossEntropy:\n",
        "  def __init__(self):\n",
        "    self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  def __call__(self, ypred, y, l2_reg=False, l=0.01, weights=[]):\n",
        "    self.l2_reg = l2_reg\n",
        "    self.l = l\n",
        "    self.weights = weights\n",
        "    ypred = torch.clamp(ypred, min=1e-10).to(self.device)\n",
        "    y = y.to(self.device)\n",
        "    loss = -torch.mean(torch.sum(y * torch.log(ypred), dim=1))\n",
        "    if l2_reg and weights:\n",
        "      loss += (l / 2) * sum(torch.sum(w ** 2) for w in weights)\n",
        "    return loss\n",
        "\n",
        "  def backward(self, ypred, y):\n",
        "    ypred = ypred.to(self.device)\n",
        "    y = y.to(self.device)\n",
        "    grad_output = -y / ypred\n",
        "    if self.l2_reg and self.weights:\n",
        "      for w in self.weights:\n",
        "        grad_output += self.l * w\n",
        "    return grad_output\n"
      ],
      "metadata": {
        "id": "ATe5r3z4qHY5"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "9SJZW5TCYXNV"
      },
      "outputs": [],
      "source": [
        "class Linear:\n",
        "  def __init__(self, input_size, output_size, activation, device=None, dropout=1.0):\n",
        "    self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') if device is None else device\n",
        "    self.input_size = input_size\n",
        "    self.output_size = output_size\n",
        "    self.w = torch.randn(output_size, input_size, device=self.device, requires_grad=True) * 0.01\n",
        "    self.b = torch.zeros(output_size, device=self.device, requires_grad=True)\n",
        "    self.activation = activation\n",
        "    self.dropout = dropout\n",
        "\n",
        "  def __call__(self, x, training=True):\n",
        "    self.x = x.to(self.device)\n",
        "    self.z = torch.matmul(self.x, self.w.T) + self.b\n",
        "    self.a = self.activation(self.z)\n",
        "\n",
        "\n",
        "    if training and self.dropout < 1.0:\n",
        "      mask = (torch.rand(self.a.shape, device=self.device) < self.dropout).float()\n",
        "      self.a *= mask\n",
        "      self.a /= self.dropout\n",
        "\n",
        "    return self.a\n",
        "\n",
        "  def backward(self, grad):\n",
        "    grad = self.activation.backward(grad)\n",
        "    dw = torch.matmul(grad.T, self.x)\n",
        "    db = torch.sum(grad, dim=0)\n",
        "    dx = torch.matmul(grad, self.w)\n",
        "    return dx, dw, db"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GradientDescent:\n",
        "  def __init__(self, learning_rate, beta):\n",
        "    self.lr = learning_rate\n",
        "    self.beta = beta\n",
        "\n",
        "  def optimize(self, dw, db, layer):\n",
        "    if not hasattr(self, 'vw') or self.vw.shape != dw.shape:\n",
        "        self.vw = torch.zeros_like(dw)\n",
        "        self.vb = torch.zeros_like(db)\n",
        "\n",
        "    self.vw = (self.beta * self.vw) + ((1 - self.beta) * dw)\n",
        "    self.vb = (self.beta * self.vb) + ((1 - self.beta) * db)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      layer.w -= self.lr * self.vw\n",
        "      layer.b -= self.lr * self.vb"
      ],
      "metadata": {
        "id": "uQGPX94B1Jow"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Adam:\n",
        "  def __init__(self, learning_rate, beta1, beta2):\n",
        "    self.lr = learning_rate\n",
        "    self.beta1 = beta1\n",
        "    self.beta2 = beta2\n",
        "\n",
        "  def optimize(self, dw, db, layer, t):\n",
        "    if not hasattr(self, 'vw') or self.vw.shape != dw.shape:\n",
        "        self.vw = torch.zeros_like(dw)\n",
        "        self.vb = torch.zeros_like(db)\n",
        "        self.sw = torch.zeros_like(dw)\n",
        "        self.sb = torch.zeros_like(db)\n",
        "\n",
        "    self.vw = (self.beta1 * self.vw) + ((1 - self.beta1) * dw)\n",
        "    self.vb = (self.beta1 * self.vb) + ((1 - self.beta1) * db)\n",
        "    self.sw = (self.beta2 * self.sw) + ((1 - self.beta2) * dw * dw)\n",
        "    self.sb = (self.beta2 * self.sb) + ((1 - self.beta2) * db * db)\n",
        "\n",
        "    self.vw = self.vw/(1-pow(self.beta1,t+1))\n",
        "    self.vb = self.vb/(1-pow(self.beta1,t+1))\n",
        "    self.sw = self.sw/(1-pow(self.beta2,t+1))\n",
        "    self.sb = self.sb/(1-pow(self.beta2,t+1))\n",
        "\n",
        "    with torch.no_grad():\n",
        "      layer.w -= self.lr * (self.vw/(torch.sqrt(self.sw) + 1e-8))\n",
        "      layer.b -= self.lr * (self.vb/(torch.sqrt(self.sb) + 1e-8))"
      ],
      "metadata": {
        "id": "m__MDiP0OxUo"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FCNN:\n",
        "  def __init__(self, dropout=False):\n",
        "    self.l1 = Linear(3072, 1024, Relu())\n",
        "    self.l2 = Linear(1024, 512, Relu())\n",
        "    self.l3 = Linear(512, 64, Relu())\n",
        "    self.l4 = Linear(64, 10, Softmax())\n",
        "    self.layers = [self.l1, self.l2, self.l3, self.l4]\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.l1(x)\n",
        "    x = self.l2(x)\n",
        "    x = self.l3(x)\n",
        "    x = self.l4(x)\n",
        "    return x\n",
        "\n",
        "  def __call__(self, x):\n",
        "    return self.forward(x)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_gN-0fQpaih4"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = torchvision.datasets.CIFAR10(root=\"/content\",download=True)\n",
        "\n",
        "xs = []\n",
        "labels = []\n",
        "\n",
        "for i in range(1,6):\n",
        "  with open(f\"/content/cifar-10-batches-py/data_batch_{i}\", 'rb') as f:\n",
        "    dict = pickle.load(f, encoding='bytes')\n",
        "    xs.append(dict[b'data'])\n",
        "    labels += dict[b'labels']\n",
        "\n",
        "\n",
        "labels = np.array(labels).reshape(-1,1)\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "y = encoder.fit_transform(labels)\n",
        "\n",
        "xs = np.array(xs)\n",
        "x = xs.reshape(50000, 3072) / 255\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    x, y, test_size=0.166, random_state=42)\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
        "\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OaZtRznWPE8F",
        "outputId": "34f8e61f-a21f-430d-8a69-d0bed6830d5a"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_lr(epoch, initial_lr=1e-3):\n",
        "    if epoch < 5:\n",
        "        return initial_lr * min(1.0, float(epoch + 1) / 5)\n",
        "    elif epoch < 30:\n",
        "        return initial_lr\n",
        "    elif epoch < 40:\n",
        "        return initial_lr * 0.1\n",
        "    else:\n",
        "        return initial_lr * 0.01"
      ],
      "metadata": {
        "id": "C39GXyYeZ0dU"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using {device}\")\n",
        "\n",
        "\n",
        "model = FCNN()\n",
        "lr = 2e-5\n",
        "\n",
        "\n",
        "loss_function = CrossEntropy()\n",
        "optimizer = Adam(learning_rate=lr, beta1=.9, beta2=.99)\n",
        "#optimizer = GradientDescent(learning_rate=1e-3, beta=.9)\n",
        "\n",
        "\n",
        "\n",
        "epochs = 50\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  losses = []\n",
        "  n_total = 0\n",
        "  n_correct = 0\n",
        "\n",
        "  optimizer.lr = get_lr(epoch, lr)\n",
        "\n",
        "  for (iteration, (x,y)) in enumerate(train_loader):\n",
        "    ypred = model(x)\n",
        "    loss = loss_function(ypred, y)\n",
        "    losses.append(loss)\n",
        "\n",
        "    n_total += batch_size\n",
        "    guesses = torch.argmax(ypred, dim=1)\n",
        "    truths = torch.argmax(y, dim=1)\n",
        "\n",
        "    for i in range(len(guesses)):\n",
        "      if(guesses[i]==truths[i]):\n",
        "        n_correct += 1\n",
        "\n",
        "\n",
        "    # Backpropagation\n",
        "    grad = loss_function.backward(ypred, y)\n",
        "    dx = grad\n",
        "\n",
        "    for layer in reversed(model.layers):\n",
        "      dx, dw, db = layer.backward(dx)\n",
        "      optimizer.optimize(dw, db, layer, iteration)\n",
        "\n",
        "  print(f\"epoch {epoch} loss: {sum(losses)/len(losses)} accuracy: {n_correct/n_total}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 805
        },
        "id": "KZ28sDhHitJL",
        "outputId": "e8b8cc03-9e09-4460-a7a9-8da778782515"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda\n",
            "epoch 0 loss: 2.2982895374298096 accuracy: 0.18570264570552147\n",
            "epoch 1 loss: 2.204258441925049 accuracy: 0.1875\n",
            "epoch 2 loss: 2.0863397121429443 accuracy: 0.19037576687116564\n",
            "epoch 3 loss: 2.0453364849090576 accuracy: 0.22136215490797545\n",
            "epoch 4 loss: 1.9854316711425781 accuracy: 0.26754217791411045\n",
            "epoch 5 loss: 1.9249241352081299 accuracy: 0.29426284509202455\n",
            "epoch 6 loss: 1.8925057649612427 accuracy: 0.31120590490797545\n",
            "epoch 7 loss: 1.8689824342727661 accuracy: 0.31731690950920244\n",
            "epoch 8 loss: 1.8494713306427002 accuracy: 0.3252971625766871\n",
            "epoch 9 loss: 1.8294126987457275 accuracy: 0.33217503834355827\n",
            "epoch 10 loss: 1.8093458414077759 accuracy: 0.34176092791411045\n",
            "epoch 11 loss: 1.7902758121490479 accuracy: 0.3482553680981595\n",
            "epoch 12 loss: 1.7712310552597046 accuracy: 0.35611579754601225\n",
            "epoch 13 loss: 1.753353476524353 accuracy: 0.3631134969325153\n",
            "epoch 14 loss: 1.7387148141860962 accuracy: 0.368888995398773\n",
            "epoch 15 loss: 1.7233991622924805 accuracy: 0.3739455521472393\n",
            "epoch 16 loss: 1.7089024782180786 accuracy: 0.37974501533742333\n",
            "epoch 17 loss: 1.6942557096481323 accuracy: 0.3851131134969325\n",
            "epoch 18 loss: 1.6816859245300293 accuracy: 0.3881566334355828\n",
            "epoch 19 loss: 1.6691991090774536 accuracy: 0.3944353911042945\n",
            "epoch 20 loss: 1.655305027961731 accuracy: 0.3987969708588957\n",
            "epoch 21 loss: 1.6414307355880737 accuracy: 0.4068730828220859\n",
            "epoch 22 loss: 1.627528429031372 accuracy: 0.410659509202454\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-129-7f73ab166595>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m       \u001b[0mdx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"epoch {epoch} loss: {sum(losses)/len(losses)} accuracy: {n_correct/n_total}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-125-0cc63bd68964>\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, dw, db, layer, t)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msb\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m       \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvw\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m       \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvb\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_func\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}