{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO1ZH1riYRqJMI9G//PO4Ni",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JacobAshoo/NNFS/blob/main/notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch"
      ],
      "metadata": {
        "id": "T-Q862boZl9J"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Relu:\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def __call__(self, x):\n",
        "    self.x = x\n",
        "    return torch.maximum(x, torch.tensor(0.0, device=x.device))\n",
        "\n",
        "  def backward(self, grad):\n",
        "    return grad * (self.x > 0).float()\n",
        "\n",
        "class Softmax:\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def __call__(self, x):\n",
        "    exp_x = torch.exp(x - torch.max(x, dim=1, keepdim=True).values)\n",
        "    self.softmax_out = exp_x / torch.sum(exp_x, dim=1, keepdim=True)\n",
        "    return self.softmax_out\n",
        "\n",
        "  def backward(self, grad):\n",
        "    batch_size, num_classes = self.softmax_out.shape\n",
        "    jacobian = torch.zeros(batch_size, num_classes, num_classes, device=grad.device)\n",
        "    for i in range(num_classes):\n",
        "      for j in range(num_classes):\n",
        "        if i == j:\n",
        "          jacobian[:, i, j] = self.softmax_out[:, i] * (1 - self.softmax_out[:, i])\n",
        "        else:\n",
        "          jacobian[:, i, j] = -self.softmax_out[:, i] * self.softmax_out[:, j]\n",
        "    return torch.bmm(jacobian, grad.unsqueeze(-1)).squeeze(-1)\n"
      ],
      "metadata": {
        "id": "_Q0ry4SqY1lG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossEntropy:\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def __call__(self, ypred, y, l2_reg=False, l=0.01, weights=[]):\n",
        "    self.l2_reg = l2_reg\n",
        "    self.l = l\n",
        "    self.weights = weights\n",
        "    ypred = torch.clamp(ypred, min=1e-9)\n",
        "    loss = -torch.mean(torch.sum(y * torch.log(ypred), dim=1))\n",
        "    if l2_reg and weights:\n",
        "      loss += (l / 2) * sum(torch.sum(w ** 2) for w in weights)\n",
        "    return loss\n",
        "\n",
        "  def backward(self, ypred, y):\n",
        "    grad_output = -y / ypred\n",
        "    if self.l2_reg and self.weights:\n",
        "      for w in self.weights:\n",
        "        grad_output += self.l * w\n",
        "    return grad_output"
      ],
      "metadata": {
        "id": "ATe5r3z4qHY5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9SJZW5TCYXNV"
      },
      "outputs": [],
      "source": [
        "class Linear:\n",
        "  def __init__(self, input_size, output_size, activation, device=None):\n",
        "    self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    self.input_size = input_size\n",
        "    self.output_size = output_size\n",
        "    self.w = torch.randn(output_size, input_size, device=self.device, requires_grad=True)\n",
        "    self.b = torch.zeros(output_size, device=self.device, requires_grad=True)\n",
        "    self.activation = activation\n",
        "\n",
        "  def __call__(self, x):\n",
        "    self.x = x\n",
        "    self.z = torch.matmul(x, self.w.T) + self.b\n",
        "    return self.activation(self.z)\n",
        "\n",
        "  def backward(self, grad):\n",
        "    grad = self.activation.backward(grad)\n",
        "    dw = torch.matmul(grad.T, self.x)\n",
        "    db = torch.sum(grad, dim=0)\n",
        "    dx = torch.matmul(grad, self.w)\n",
        "    return dx, dw, db"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FCNN:\n",
        "  def __init__(self, input_size, output_size, dropout=False):\n",
        "    self.l1 = Linear(input_size, 10, Relu())\n",
        "    self.l2 = Linear(10, output_size, Softmax())\n",
        "    self.layers = [self.l1, self.l2]\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.l1(x)\n",
        "    x = self.l2(x)\n",
        "    return x\n",
        "\n",
        "  def __call__(self, x):\n",
        "    return self.forward(x)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_gN-0fQpaih4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using {device}\")\n",
        "\n",
        "model = FCNN(10, 3)\n",
        "\n",
        "x = torch.randn(1, 10, device=device) * 0.01\n",
        "y = torch.tensor([[1,0,0]], device=device)\n",
        "loss_function = CrossEntropy();\n",
        "ypred = model(x)\n",
        "\n",
        "print(ypred)\n",
        "loss = loss_function(ypred, y)\n",
        "print(loss)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZ28sDhHitJL",
        "outputId": "7e427be3-7ff7-483e-d3d4-a559b5c407ff"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu\n",
            "tensor([[0.3416, 0.3423, 0.3161]], grad_fn=<DivBackward0>)\n",
            "tensor(1.0741, grad_fn=<NegBackward0>)\n"
          ]
        }
      ]
    }
  ]
}