{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOGMtgCJP89d/hgRE/06cMr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JacobAshoo/NNFS/blob/main/notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dill\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision.datasets\n",
        "import dill\n",
        "import pickle\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn.functional as F\n",
        "from math import cos, pi\n",
        "\n",
        "torch.manual_seed(42)\n"
      ],
      "metadata": {
        "id": "T-Q862boZl9J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed6d74df-edc6-40f4-e11f-2879986b470e"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (0.3.9)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7b176879e370>"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Relu:\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def __call__(self, x):\n",
        "    self.x = x\n",
        "    return torch.maximum(x, torch.tensor(0.0, device=x.device))\n",
        "\n",
        "  def backward(self, grad):\n",
        "    return grad * (self.x > 0).float()\n",
        "\n",
        "\n",
        "\n",
        "class Softmax:\n",
        "  def __init__(self, dim=-1, device=None):\n",
        "    self.dim = dim\n",
        "    self.device = device if device else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  def __call__(self, x):\n",
        "    x = x.to(self.device)\n",
        "    x_max = torch.max(x, dim=self.dim, keepdim=True)[0]\n",
        "    exp_x = torch.exp(x - x_max)\n",
        "    sum_exp_x = torch.sum(exp_x, dim=self.dim, keepdim=True)\n",
        "    self.softmax_output = exp_x / sum_exp_x\n",
        "    return self.softmax_output\n",
        "\n",
        "  def backward(self, grad_output):\n",
        "    batch_size, num_classes = self.softmax_output.shape\n",
        "    eye = torch.eye(num_classes, device=self.device).unsqueeze(0)\n",
        "    softmax_diag = self.softmax_output.unsqueeze(2) * eye\n",
        "    softmax_outer = torch.matmul(self.softmax_output.unsqueeze(2), self.softmax_output.unsqueeze(1))\n",
        "    jacobian = softmax_diag - softmax_outer\n",
        "    grad_input = torch.matmul(jacobian, grad_output.unsqueeze(2)).squeeze(2)\n",
        "    return grad_input\n",
        "\n"
      ],
      "metadata": {
        "id": "_Q0ry4SqY1lG"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossEntropy:\n",
        "  def __init__(self):\n",
        "    self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  def __call__(self, ypred, y, l2_reg=False, l=0.01, weights=[]):\n",
        "    self.l2_reg = l2_reg\n",
        "    self.l = l\n",
        "    self.weights = weights\n",
        "    ypred = torch.clamp(ypred, min=1e-10).to(self.device)\n",
        "    y = y.to(self.device)\n",
        "    loss = -torch.mean(torch.sum(y * torch.log(ypred), dim=1))\n",
        "    if l2_reg and weights:\n",
        "      loss += (l / 2) * sum(torch.sum(w ** 2) for w in weights)\n",
        "    return loss\n",
        "\n",
        "  def backward(self, ypred, y):\n",
        "    ypred = ypred.to(self.device)\n",
        "    y = y.to(self.device)\n",
        "    grad_output = -y / (ypred+1e-8)\n",
        "    if self.l2_reg and self.weights:\n",
        "      for w in self.weights:\n",
        "        grad_output += self.l * w\n",
        "    return grad_output\n"
      ],
      "metadata": {
        "id": "ATe5r3z4qHY5"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "9SJZW5TCYXNV"
      },
      "outputs": [],
      "source": [
        "class Linear:\n",
        "  def __init__(self, input_size, output_size, activation, device=None, dropout=1.0):\n",
        "    self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') if device is None else device\n",
        "    self.input_size = input_size\n",
        "    self.output_size = output_size\n",
        "    self.w = torch.randn(output_size, input_size, device=self.device) * .1\n",
        "    self.b = torch.zeros(output_size, device=self.device)\n",
        "    self.activation = activation\n",
        "    self.dropout = dropout\n",
        "\n",
        "  def __call__(self, x, training=True):\n",
        "    self.x = x.to(self.device)\n",
        "    self.z = torch.matmul(self.x, self.w.T) + self.b\n",
        "    self.a = self.activation(self.z)\n",
        "\n",
        "\n",
        "    if training and self.dropout < 1.0:\n",
        "      mask = (torch.rand(self.a.shape, device=self.device) < self.dropout).float()\n",
        "      self.a *= mask\n",
        "      self.a /= self.dropout\n",
        "\n",
        "    return self.a\n",
        "\n",
        "  def backward(self, grad):\n",
        "    grad = self.activation.backward(grad)\n",
        "    dw = torch.matmul(grad.T, self.x)\n",
        "    db = torch.sum(grad, dim=0)\n",
        "    dx = torch.matmul(grad, self.w)\n",
        "    return dx, dw, db"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Conv2D:\n",
        "  def __init__(self, in_channels, out_channels, kernel_size, activation, stride=1):\n",
        "    self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    self.in_channels = in_channels\n",
        "    self.out_channels = out_channels\n",
        "    self.kernel_size = kernel_size\n",
        "    self.activation = activation\n",
        "    self.stride = stride\n",
        "    self.w = torch.randn(out_channels, in_channels, kernel_size, kernel_size, device=self.device) * 0.1\n",
        "    self.b = torch.zeros(out_channels, device=self.device)\n",
        "    self.original_shape = None\n",
        "\n",
        "  def __call__(self, x):\n",
        "    return self.forward(x)\n",
        "\n",
        "  def forward(self, x):\n",
        "    self.original_shape = x.shape  # Store original shape\n",
        "    self.x = x.to(self.device)  # Move input to the correct device\n",
        "    batch_size, in_channels, in_height, in_width = x.shape\n",
        "\n",
        "    out_height = (in_height - self.kernel_size) // self.stride + 1\n",
        "    out_width = (in_width - self.kernel_size) // self.stride + 1\n",
        "\n",
        "    x_unfolded = F.unfold(self.x, kernel_size=self.kernel_size, stride=self.stride).to(self.device)  # Move to device\n",
        "    w_reshaped = self.w.view(self.out_channels, -1)\n",
        "\n",
        "    out = torch.matmul(w_reshaped, x_unfolded) + self.b.view(-1, 1)\n",
        "    out = out.view(batch_size, self.out_channels, out_height, out_width)\n",
        "    self.a = self.activation(out)\n",
        "    return self.a\n",
        "\n",
        "\n",
        "  def backward(self, grad):\n",
        "    if grad.dim() == 2:\n",
        "      batch_size = self.original_shape[0]\n",
        "      out_height = (self.original_shape[2] - self.kernel_size) // self.stride + 1\n",
        "      out_width = (self.original_shape[3] - self.kernel_size) // self.stride + 1\n",
        "      grad = grad.view(batch_size, self.out_channels, out_height, out_width)\n",
        "\n",
        "    batch_size, _, out_height, out_width = grad.shape\n",
        "    grad_flattened = grad.view(batch_size, self.out_channels, -1)\n",
        "    x_unfolded = F.unfold(self.x, kernel_size=self.kernel_size, stride=self.stride)\n",
        "\n",
        "    dw = torch.matmul(grad_flattened, x_unfolded.transpose(1, 2))\n",
        "    dw = dw.sum(dim=0).view(self.w.shape)\n",
        "    db = grad_flattened.sum(dim=(0, 2))\n",
        "\n",
        "    w_reshaped = self.w.view(self.out_channels, -1)\n",
        "    dx_unfolded = torch.matmul(w_reshaped.T, grad_flattened)\n",
        "    dx = F.fold(dx_unfolded, output_size=(self.x.shape[2], self.x.shape[3]), kernel_size=self.kernel_size, stride=self.stride)\n",
        "\n",
        "    return dx, dw, db\n"
      ],
      "metadata": {
        "id": "TV9y6BR3WvuK"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GradientDescent:\n",
        "  def __init__(self, learning_rate, beta):\n",
        "    self.lr = learning_rate\n",
        "    self.beta = beta\n",
        "\n",
        "  def optimize(self, dw, db, layer):\n",
        "    if not hasattr(self, 'vw') or self.vw.shape != dw.shape:\n",
        "        self.vw = torch.zeros_like(dw)\n",
        "        self.vb = torch.zeros_like(db)\n",
        "\n",
        "    self.vw = (self.beta * self.vw) + ((1 - self.beta) * dw)\n",
        "    self.vb = (self.beta * self.vb) + ((1 - self.beta) * db)\n",
        "\n",
        "    layer.w -= self.lr * self.vw\n",
        "    layer.b -= self.lr * self.vb"
      ],
      "metadata": {
        "id": "uQGPX94B1Jow"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Adam:\n",
        "  def __init__(self, learning_rate, beta1, beta2):\n",
        "    self.lr = learning_rate\n",
        "    self.beta1 = beta1\n",
        "    self.beta2 = beta2\n",
        "    self.t = 1\n",
        "\n",
        "  def optimize(self, dw, db, layer):\n",
        "    if not hasattr(self, 'vw') or self.vw.shape != dw.shape:\n",
        "        self.vw = torch.zeros_like(dw)\n",
        "        self.vb = torch.zeros_like(db)\n",
        "        self.sw = torch.zeros_like(dw)\n",
        "        self.sb = torch.zeros_like(db)\n",
        "\n",
        "    self.vw = (self.beta1 * self.vw) + ((1 - self.beta1) * dw)\n",
        "    self.vb = (self.beta1 * self.vb) + ((1 - self.beta1) * db)\n",
        "    self.sw = (self.beta2 * self.sw) + ((1 - self.beta2) * dw * dw)\n",
        "    self.sb = (self.beta2 * self.sb) + ((1 - self.beta2) * db * db)\n",
        "\n",
        "    self.vw = self.vw/(1-pow(self.beta1,self.t))\n",
        "    self.vb = self.vb/(1-pow(self.beta1,self.t))\n",
        "    self.sw = self.sw/(1-pow(self.beta2,self.t))\n",
        "    self.sb = self.sb/(1-pow(self.beta2,self.t))\n",
        "\n",
        "\n",
        "    layer.w -= self.lr * (self.vw/(torch.sqrt(self.sw) + 1e-8))\n",
        "    layer.b -= self.lr * (self.vb/(torch.sqrt(self.sb) + 1e-8))\n",
        "\n",
        "    self.t += 1\n"
      ],
      "metadata": {
        "id": "m__MDiP0OxUo"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def step_decay(epoch, initial_lr=1e-3):\n",
        "    if epoch < 3:\n",
        "        return initial_lr * min(1.0, float(epoch + 1) / 5)\n",
        "    elif epoch < 10:\n",
        "        return initial_lr\n",
        "    elif epoch < 15:\n",
        "        return initial_lr * 0.5\n",
        "    else:\n",
        "        return initial_lr * 0.01\n",
        "\n",
        "def cosine_decay(epoch, num_epochs, initial_lr):\n",
        "  return .5 *initial_lr * (1 + cos((epoch * pi)/num_epochs))\n",
        "\n",
        "def warmup(epoch, lr):\n",
        "  if epoch < 5:\n",
        "    return .1 * lr\n",
        "  else:\n",
        "    return lr"
      ],
      "metadata": {
        "id": "C39GXyYeZ0dU"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FCNN:\n",
        "  def __init__(self):\n",
        "    self.l1 = Conv2D(3, 32, 3, Relu(), stride=3)\n",
        "    self.l2 = Conv2D(32, 64, 3, Relu(), stride=3)\n",
        "    self.l3 = Linear(576, 576, Relu())\n",
        "    self.l4 = Linear(576, 10, Softmax())\n",
        "\n",
        "    self.layers = [self.l1, self.l2, self.l3, self.l4]\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.l1(x)\n",
        "    x = self.l2(x)\n",
        "    x = torch.flatten(x, 1)\n",
        "    x = self.l3(x)\n",
        "    x = self.l4(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "  def __call__(self, x):\n",
        "    return self.forward(x)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_gN-0fQpaih4"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(model, name):\n",
        "  with open(f\"{name}.dill\", \"wb\") as f:\n",
        "    dill.dump(model, f)\n",
        "\n"
      ],
      "metadata": {
        "id": "XZE0SfjOx0Z8"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = torchvision.datasets.CIFAR10(root=\"/content\",download=True)\n",
        "\n",
        "xs = []\n",
        "labels = []\n",
        "\n",
        "for i in range(1,6):\n",
        "  with open(f\"/content/cifar-10-batches-py/data_batch_{i}\", 'rb') as f:\n",
        "    dict = pickle.load(f, encoding='bytes')\n",
        "    xs.append(dict[b'data'])\n",
        "    labels += dict[b'labels']\n",
        "\n",
        "\n",
        "labels = np.array(labels).reshape(-1,1)\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "y = encoder.fit_transform(labels)\n",
        "\n",
        "xs = np.array(xs)\n",
        "x = xs.reshape(50000, 3, 32, 32) / 255\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    x, y, test_size=0.166, random_state=42)\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
        "\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "batch_size = 32\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "X_train = X_train.to(device)\n",
        "y_train = y_train.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OaZtRznWPE8F",
        "outputId": "dcac71c3-76b2-460e-93ce-8183f86f9a78"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title train\n",
        "\n",
        "\n",
        "def train(model, epochs, lr, loss_function, optimizer, name, decay_algo=None):\n",
        "  for epoch in range(epochs):\n",
        "    losses = []\n",
        "    n_total = 0\n",
        "    n_correct = 0\n",
        "\n",
        "    if decay_algo == \"cosine\":\n",
        "      optimizer.lr = cosine_decay(epoch, epochs, lr)\n",
        "    if decay_algo == \"step\":\n",
        "      optimizer.lr = step_decay(epoch, lr)\n",
        "    if(decay_algo==\"warmup\"):\n",
        "      optimizer.lr = warmup(epoch, lr)\n",
        "\n",
        "    for (iteration, (x,y)) in enumerate(train_loader):\n",
        "      ypred = model(x)\n",
        "      loss = loss_function(ypred, y)\n",
        "      losses.append(loss)\n",
        "\n",
        "      n_total += batch_size\n",
        "      guesses = torch.argmax(ypred, dim=1)\n",
        "      truths = torch.argmax(y, dim=1)\n",
        "\n",
        "      for i in range(len(guesses)):\n",
        "        if(guesses[i]==truths[i]):\n",
        "          n_correct += 1\n",
        "\n",
        "\n",
        "      # Backpropagation\n",
        "      grad = loss_function.backward(ypred, y)\n",
        "      dx = grad\n",
        "\n",
        "      for layer in reversed(model.layers):\n",
        "        dx, dw, db = layer.backward(dx)\n",
        "        optimizer.optimize(dw, db, layer)\n",
        "\n",
        "    print(f\"epoch {epoch} loss: {sum(losses)/len(losses)} accuracy: {n_correct/n_total}\")\n",
        "  save_model(model, name)\n",
        "\n"
      ],
      "metadata": {
        "id": "KZ28sDhHitJL"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Ablation\n",
        "print(f\"Using {device}\")\n",
        "model = FCNN()\n",
        "lr = 5e-3\n",
        "loss_function = CrossEntropy()\n",
        "epochs = 20\n",
        "\n",
        "\n",
        "\n",
        "#basline adam and gcd no decay\n",
        "#train(model, epochs, lr, loss_function, GradientDescent(learning_rate=lr, beta=.9), \"Default_GD\")\n",
        "model = FCNN()\n",
        "train(model, epochs, lr, loss_function, Adam(learning_rate=lr, beta1=.9, beta2=.999), \"Default_ADAM\", decay_algo=\"cosine\")\n",
        "exit()\n",
        "model = FCNN()\n",
        "\n",
        "#adam and gcd with step and cosine\n",
        "train(model, epochs, lr, loss_function, GradientDescent(learning_rate=lr, beta=.9), \"Default_GD_Step\",decay_algo=\"step\")\n",
        "model = FCNN()\n",
        "train(model, epochs, lr, loss_function, GradientDescent(learning_rate=lr, beta=.9), \"Default_GD_Cosine\",decay_algo=\"cosine\")\n",
        "model = FCNN()\n",
        "train(model, epochs, lr, loss_function, Adam(learning_rate=lr, beta1=.9, beta2=.99), \"Default_Adam_Step\",decay_algo=\"step\")\n",
        "model = FCNN()\n",
        "train(model, epochs, lr, loss_function, Adam(learning_rate=lr, beta1=.9, beta2=.99), \"Default_Adam_Cosine\",decay_algo=\"cosine\")\n",
        "model = FCNN()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vOFFzqEhq6tE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 569
        },
        "outputId": "1e38c750-857a-4f6d-abf3-8feee045e2e4"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu\n",
            "epoch 0 loss: 18.36421775817871 accuracy: 0.1058282208588957\n",
            "epoch 1 loss: 20.729246139526367 accuracy: 0.0997411809815951\n",
            "epoch 2 loss: 20.725372314453125 accuracy: 0.0997411809815951\n",
            "epoch 3 loss: 20.729246139526367 accuracy: 0.0997411809815951\n",
            "epoch 4 loss: 20.72924041748047 accuracy: 0.0997411809815951\n",
            "epoch 5 loss: 20.729236602783203 accuracy: 0.0997411809815951\n",
            "epoch 6 loss: 20.729209899902344 accuracy: 0.0997411809815951\n",
            "epoch 7 loss: 20.725399017333984 accuracy: 0.0997411809815951\n",
            "epoch 8 loss: 20.729232788085938 accuracy: 0.0997411809815951\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-92-c7554c1002bc>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#train(model, epochs, lr, loss_function, GradientDescent(learning_rate=lr, beta=.9), \"Default_GD\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.999\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Default_ADAM\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay_algo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cosine\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-91-3a5dd38f1b6a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, epochs, lr, loss_function, optimizer, name, decay_algo)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m       \u001b[0mypred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mypred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-88-300d4c871580>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-88-300d4c871580>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-84-5d229dbe93d2>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-84-5d229dbe93d2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mw_reshaped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_reshaped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_unfolded\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_width\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#adam with diferent betas and best decay function\n",
        "train(model, epochs, lr, loss_function, Adam(learning_rate=lr, beta1=.7, beta2=.99), \"Adam\")\n",
        "model = FCNN()\n",
        "train(model, epochs, lr, loss_function, Adam(learning_rate=lr, beta1=.9, beta2=.8))\n",
        "model = FCNN()\n",
        "\n"
      ],
      "metadata": {
        "id": "1ukvjTVKVHIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#regularization on best model so far\n",
        "#dropout and l2 reg diferent lambdas"
      ],
      "metadata": {
        "id": "LuV-f6QN0uvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "svo0v816WXpx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}